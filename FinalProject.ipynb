{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6738599",
   "metadata": {},
   "source": [
    "**Drew Tatum** <br>\n",
    "**Final Project** <br>\n",
    "**CSC 575 Intelligent Information Retrieval** <br>\n",
    "**Winter 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbada7f",
   "metadata": {},
   "source": [
    "## DePaul Course Catalog Information Retrieval System\n",
    "\n",
    "<ol>\n",
    "    <li><a href=#Intro>Introduction</a></li>\n",
    "    <li><a href=#WC>Web Crawler</a></li>\n",
    "    <li><a href=#Index>Indexing</a></li>\n",
    "    <li><a href=#Retrieval>Information Retrieval System</a></li>   \n",
    "    <li><a href=#Test>Test Case</a></li>  \n",
    "    <li><a href=#Results>Results</a></li>      \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e881b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import webbrowser  \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f3f57",
   "metadata": {},
   "source": [
    "## Introduction <a id=Intro>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296420f",
   "metadata": {},
   "source": [
    "The purpose of this project  to improve DePaul’s course information retrieval system from the perspective of a student taking courses at DePaul University. There are a few limitations right now that exist when using DePaul’s Campus Connect to search for courses to register. As of Winter 2021, queries that only match either the course ID or terms in the course’s title will return the course. For example, the query “SQL” will only return courses that have SQL within the course name regardless if the term is within the course description. Another limitation is the system uses an exact term match when searching its system. Since the current system doesn’t implement any type of stemming or lemmatization queries that are misspelled or are the plural form of a word like “regressions” won’t retrieve courses that have the term “regression” in it’s course title. One last notable restriction of the current retrieval system is that multiple topic’s can’t be searched at the same time. For example the query “SQL and Regression” looks for courses that both have SQL and regression within the course title. If no course exists, which is currently true, no courses are returned. Even though a few courses have regression and SQL within their course title separately, the current system isn’t able to provide the results the user desires. The goal of this information retrieval system is to help fix these current issues that are impacting student’s from exploring different courses and note of further potential development to improve their experience. Additionally the new information retrieval system implements a weighted similarity scheme based on the location of the queried term. For example, if the term appears in the course title it will be given a higher weighted value than terms appearing in the course description. This is based on the intuition that a course’s title will provide the main intellectual focus of the topic while the course description can provide topics that are briefly discussed. The objective of this weighing scheme is to provide the user results closer to their true desired query output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96ed63",
   "metadata": {},
   "source": [
    "## Web Crawler <a id=WC>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e7561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Finding what courses to add to index based on Data Science Computational Methods program\n",
    "data_science_url = 'https://www.cdm.depaul.edu/academics/Pages/Current/Requirements-MS-In-Data-Science-Computational-Methods.aspx'\n",
    "url = requests.get(data_science_url)\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "class_ids = soup.find_all(class_='CDMExtendedCourseInfo')\n",
    "additional_classes = soup.find_all(class_='manualSimplePopup')\n",
    "all_ids = class_ids + additional_classes\n",
    "\n",
    "course_ids = []\n",
    "course_pattern = re.compile('>(.*)<')\n",
    "\n",
    "for c_id in all_ids:\n",
    "    name = re.findall(course_pattern, str(c_id))[0]\n",
    "    if len(name) < 8 and name not in course_ids:  # len is to not include courses that aren't available anymore since 'no longer offered' appears with course id\n",
    "        course_ids.append(name)\n",
    "    \n",
    "##### Web scraping each individual course\n",
    "courses = {}\n",
    "courseID_docID = {}  # CourseID mapping\n",
    "for index, course in enumerate(course_ids):\n",
    "    course_num = course[-3:]\n",
    "    course_sub = course[:3].strip()\n",
    "    course_url = 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=' + course_sub + '&CatalogNbr=' + course_num \n",
    "    # Update Mapping\n",
    "    courseID_docID[int(index)] = {'Course ID': course, 'URL': course_url}\n",
    "    # Each URL now has relevant course number and subject\n",
    "    url = requests.get(course_url)\n",
    "    \n",
    "    # Parsing URL\n",
    "    soup = BeautifulSoup(url.content, 'html.parser')\n",
    "    name_results = soup.find_all(class_='PageTitle')\n",
    "    info_results = soup.find_all('p')\n",
    "    course_raw_info = info_results[1]\n",
    "    # Course information and course name patterns\n",
    "    info_pattern = re.compile('<p>\\s+(.*)</p>')\n",
    "    name_pattern = re.compile('\"PageTitle\">\\s+(\\w+)\\s+(\\d+):\\s+(.+)<\\/h1>')\n",
    "    \n",
    "    course_info = re.findall(info_pattern, str(course_raw_info))[0]\n",
    "    course_name = re.findall(name_pattern, str(name_results))[0]\n",
    "   \n",
    "    course_id = course_name[0] + course_name[1]  # Course ID is the department + 3 number representation for the class\n",
    "    course_topic = course_name[2]\n",
    "        \n",
    "    courses[course_id] = {'Topic': course_topic, 'Info': course_info}\n",
    "    \n",
    "    \n",
    "# Saving json format of dictionary for further linguistic processing\n",
    "with open('courses.json', 'w') as outfile:\n",
    "    json.dump(courses, outfile)\n",
    "\n",
    "with open('docID_mapping.json', 'w') as outfile2: # Dictionary with docID matching to course ID and url \n",
    "    json.dump(courseID_docID, outfile2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34314529",
   "metadata": {},
   "source": [
    "The above code goes through DePaul's Data Science Computational Methods program and creates a list of each course ID and course number. With this each course is crawled and a dictionary caleld courses stores the course ID alongside with the course topic and course information. The course topic and course information are separated so that they can be processed differently since different term weights will be applied to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eee8391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IT403': {'Topic': 'Statistics and Data Analysis',\n",
       "  'Info': 'Introduction to univariate data analysis methods. Descriptive statistics and data visualization methods. Overview of sampling techniques for data collection, and introduction to statistical inference methods for decision making including simple linear regression, estimation procedures using confidence intervals and hypothesis testing. PREREQUISITE(S): None'},\n",
       " 'CSC412': {'Topic': 'Tools and Techniques for Computational Analysis',\n",
       "  'Info': 'Use of mathematical software to explore basic concepts in linear algebra and calculus.  Scripting for symbolic and computational processing.  Emphasis is on applications in computer science, finance, data mining, and computer vision. PREREQUISITE(S): None'},\n",
       " 'CSC401': {'Topic': 'Introduction to Programming',\n",
       "  'Info': 'An introduction to programming with a focus on problem solving, structured programming, and algorithm design. Concepts covered include data types, expressions, variables, assignments, conditional and iterative structures, functions, file input/output, exceptions, arrays and an introduction to user-defined classes.'},\n",
       " 'DSC450': {'Topic': 'Database Processing for Large-Scale Analytics',\n",
       "  'Info': 'The course covers core concepts of database systems with focus on applications in large-scale analytics. Topics include relational databases, scheme normalization, SQL queries for data integration and data cleaning, database programming for ETL, and nontraditional database systems for unstructured data.'},\n",
       " 'DSC423': {'Topic': 'Data Analysis and Regression',\n",
       "  'Info': 'Multiple regression and correlation, residual analysis, analysis of variance, and robustness. These topics will be studied from a data analytic perspective, supported by an investigation of available statistical software.'},\n",
       " 'DSC424': {'Topic': 'Advanced Data Analysis',\n",
       "  'Info': 'The course will teach advanced statistical techniques to discover information from large sets of data.  The course topics include visualization techniques to summarize and display high dimensional data, dimensional reduction techniques such as principal component analysis and factor analysis, clustering techniques for discovering patterns from large datasets, and classification techniques for decision making.  The methods will be implemented using standard computer packages.'},\n",
       " 'DSC430': {'Topic': 'Python Programming',\n",
       "  'Info': 'This course builds the skills necessary to use Python to develop larger programs and libraries. Students will learn to design, implement and debug Python functions and programs, including stochastic and object-oriented techniques. The course will cover Python data structures, and Python facilities for working with files, strings, regular expressions, databases and URLs. The course will also include an introduction to the Pandas package for data management, the NumPy package for scientific computing, and the Matplotlib package for visualization.'},\n",
       " 'DSC441': {'Topic': 'Fundamentals of Data Science',\n",
       "  'Info': 'An introduction to the Knowledge Discovery Technologies covering all stages of a data mining process: domain understanding, data collection and selection, data cleaning and transformation, dimensionality reduction, pattern discovery, evaluation, and knowledge extraction. The course provides a comprehensive overview of data mining techniques used to realize these stages, including traditional statistical analysis and machine learning techniques. Students will analyze large datasets and develop modeling solutions to support decision making in various domains such as healthcare, finance, security, marketing, customer relationship management (CRM), and multimedia.'},\n",
       " 'DSC465': {'Topic': 'Data Visualization',\n",
       "  'Info': 'An introduction to data visualization techniques to enhance the exploration and analysis of large data sets from a wide range of fields including commercial, financial, medical, scientific and engineering applications. Topics include visual encoding of numeric data, graphical integrity and effective visualization design, visualizing distributions and correlation, false-color techniques for feature extraction and enhancement, basic network visualization and graph layout, isosurface generation, geospatial visualization and volumetric rendering techniques. The course explores both existing visualization software packages and code interfaces for data visualization.'},\n",
       " 'DSC480': {'Topic': 'Social Network Analysis',\n",
       "  'Info': 'This course is an introduction to the concepts and methods of social network analysis. Students will learn to extract and manage data about network structure and dynamics, and to analyze, model and visualize such data. Students will use software tools to model and visualize network structure and dynamics. Specific network applications to be discussed include online social networks, collaboration networks, and communication networks.'},\n",
       " 'DSC478': {'Topic': 'Programming Machine Learning Applications',\n",
       "  'Info': 'The course will focus on the implementations of various data mining and machine learning techniques using a high-level programming language.  Students will have hands on experience developing both supervised and unsupervised machine learning algorithms and will learn how to employ these techniques in the context of popular applications including automatic personalization, recommender systems, searching and ranking, text mining, group and community discovery, and social media analytics.'},\n",
       " 'CSC555': {'Topic': 'Mining Big Data',\n",
       "  'Info': 'Introduction to fundamentals of distributed file systems and map-reduce technology (e.g., Hadoop); tuning map-reduce performance in a distributed network. Algorithms and tools for mining massive data sets and discussion of current challenges. Applications in clustering, similarity search, classification, data warehousing (e.g., Hive), machine learning (e.g., Mahout).'},\n",
       " 'DSC540': {'Topic': 'Advanced Machine Learning',\n",
       "  'Info': 'The course is for students with prior background in data mining or machine learning techniques, and covers more advanced modeling techniques, including ensemble learning, extended linear models such as support vector machines, probabilistic graphical models, mixture and latent variable models, matrix factorization and link analysis.  Application of the models will be presented in popular domains such as Web and social media analytics, text mining, crime analysis, community discovery, and health informatics.'},\n",
       " 'CSC521': {'Topic': 'Monte Carlo Algorithms',\n",
       "  'Info': 'A course about the use of random numbers for numerical computation with particular emphasis on implementation issues and applications in science and finance. Covered topics include: pseudo random number generators, the inversion method, the accept-reject method, discrete event simulations, multi-dimensional integration, the Metropolis and the Bootstrap algorithms.'},\n",
       " 'CSC575': {'Topic': 'Intelligent Information Retrieval',\n",
       "  'Info': 'Examination of the design, implementation, and evaluation of information retrieval systems. The focus is on the underlying retrieval models, algorithms, and system implementations. Also examined is how an effective information search and retrieval is interrelated with the organization and description of information to be retrieved. Topics include: automatic indexing; thesaurus generation; Boolean, vector-space, and probabilistic models; clustering and classification; information filtering; distributed IR on the WWW; intelligent information agents; IR system evaluation; information visualization; and natural language processing in IR. Throughout the course, current literature from the viewpoints of both research and practical retrieval technologies both on and off the World Wide Web will be examined.  PREREQUISITE(S):  CSC 403'},\n",
       " 'CSC578': {'Topic': 'Neural Networks and Deep Learning',\n",
       "  'Info': 'Course focuses on the algorithms, implement, and application of neural networks for learning about data. It will present how neural networks represent data and learn in supervised and unsupervised contexts with applications to language processing, classification, and regression problems. Topics include learning algorithms, and optimization methods, deep learning methods for deriving deep representations from surface features, recursive networks, Boltzmann machines and convolutional networks.'},\n",
       " 'DSC425': {'Topic': 'Time Series Analysis and Forecasting',\n",
       "  'Info': 'The course introduces students to statistical models for time series analysis and forecasting. The course topics include: autocorrelated data analysis, Box-Jenkins models (autoregressive, moving average, and autoregressive moving average models), analysis of seasonality, volatility models (GARCH-type, GARCH-M type, etc.), forecasting evaluation and diagnostics checking. The course will emphasize applications to financial data, volatility modeling and risk management. Real examples will be used throughout the course.'},\n",
       " 'DSC433': {'Topic': 'Scripting for Data Analysis',\n",
       "  'Info': 'Data access and transformation with modern statistical software such as SAS and R.  Report writing, data graphing and visualization, writing macros and functions to automate tasks and statistical analyses.'},\n",
       " 'CSC452': {'Topic': 'Database Programming',\n",
       "  'Info': 'Programming in a large-scale relational database environment using procedural languages. Topics covered in the course include: procedural extension of query languages, runtime error handling, subprograms (procedures and functions), packages, database triggers, dynamic query language. Optional topics include transaction management, reliability, and security.'},\n",
       " 'CSC481': {'Topic': 'Introduction to Image Processing',\n",
       "  'Info': 'The course is a prerequisite for more advanced Visual Computing (VC) courses and the students will be challenged to implement VC algorithms for real world applications.  The topics covered in the course include: components of an image processing system and its applications, elements of visual perception, sampling and quantization, image enhancement by histogram equalization, color spaces and transformations, introduction to segmentation (Edge detection), and morphological image processing.  PREREQUISITE(S): CSC 412 or consent of instructor'},\n",
       " 'CSC482': {'Topic': 'Applied Image Analysis',\n",
       "  'Info': 'Image analysis from classical computational imaging techniques to deep learning techniques. Fundamentals of computational image analysis in terms of image information extraction and modeling of image patterns. Specific topics include, but are not limited to: image segmentation, multi-scale representation, shape analysis, texture analysis, Fourier analysis, wavelets, Gabor and fractal analysis, and template matching. Deep learning models to extract image representations automatically. Classical and deep learning imaging techniques applied and compared in the context of different image analysis tasks such as image representation, segmentation, classification, retrieval, and object recognition. Applications of these techniques for autonomous driving, biometrics, sports analytics, smart and connected communities, and biomedical and health informatics. PREREQUISITE(S) CSC 481'},\n",
       " 'DSC484': {'Topic': 'Web Data Mining',\n",
       "  'Info': 'An in-depth study of the knowledge discovery process and its applications in Web mining, Web analytics and business intelligence.  The course provides coverage of various aspects of data collection and preprocessing, as well as basic data mining techniques for segmentation, classification, predictive modeling, association analysis, and sequential pattern discovery.  The primary focus of the course is the application of these techniques to Web analytics, user behavior modeling, e-metrics for business intelligence, Web personalization and recommender systems.  Also addressed are privacy and ethical issues related to Web data mining.  Students can choose from three types of final course projects: implementation projects, research papers, or data analysis projects.  Throughout the course, the students will learn and use a variety of data mining tools to analyze sample data sets as part of class assignments.'},\n",
       " 'CSC528': {'Topic': 'Computer Vision',\n",
       "  'Info': 'Introduction to computer vision, including fundamentals of image formation, camera imaging geometry, feature detection and matching, stereo, and motion analysis. Basic methods and modern deep learning models for applications that include camera calibration, depth recovery from video, motion estimation and tracking, image and video, image search, and new image generation. Focus on the mathematics of these methods and models.  Development of modern techniques such as convolutional neural networks for optical flow estimation and object detection, and generative adversarial networks for artificial image generation. PREREQUISITE(S): CSC 481'},\n",
       " 'DSC510': {'Topic': 'Health Data Science',\n",
       "  'Info': 'The course will focus on data science methods used in clinical studies and public health applications. Students will be introduced to a variety of health care data from electronic health records to payer data, geospational and unstructured data, and will learn how to solve data science problems in the health sector. Topics include overview of healthcare analytics and typical research questions, epidemiology, data ethics, governance and security, applications of modeling techniques and machine learning methods to a variety of case studies in health care.'},\n",
       " 'CSC543': {'Topic': 'Spatial Databases and Geographic Information Systems',\n",
       "  'Info': 'This course considers how spatial databases work within a GIS, how data is structured, stored, indexed, retrieved, and displayed.  Other topics may include fuzzy spatial databases, temporal spatial databases, and multiple criteria spatial decision making. The class will consist of hands-on work with commercial products, as well as investigating the state of-the art research in the field.  Prerequisites: CSC 453 or CSC 455'},\n",
       " 'CSC576': {'Topic': 'Computational Advertising',\n",
       "  'Info': 'Computational advertising is the problem of finding the best advertisement for a given user in a given on-line context. It is a complex and emerging area at the intersection of quantitative marketing, web search, data mining, recommendation, optimization, and algorithmic game theory. Students will read current scientific papers and explore a range of models both mathematically and empirically. Students can choose from three types of final course projects: implementation projects, research papers, or data analysis projects. Prerequisite(s): IS 467 or CSC 478 or ECT 584'},\n",
       " 'CSC577': {'Topic': 'Recommender Systems',\n",
       "  'Info': 'Recommender systems offer personalized access to online information in product catalogs, social media networks, and document collections, among other applications. This class will introduce students to a range of approaches for building recommender systems including collaborative, content-based, knowledge-based, and hybrid methods. Students will implement recommendation algorithms using an open-sourced toolkit and conduct experimental evaluations.  PREREQUISITE(S): (CSC 412 and CSC 478) or (CSC 403 and IS 467)'},\n",
       " 'CSC594': {'Topic': 'Topics in Artificial Intelligence',\n",
       "  'Info': 'Specific topics will be selected by the instructor and may vary with each quarter. Can be repeated for credit. Variable credit.  PREREQUISITE(S): For specific prerequisites, see syllabus or consult course instructor. (variable credit)'},\n",
       " 'CSC598': {'Topic': 'Topics in Data Analysis',\n",
       "  'Info': 'Specific topics will be selected by the instructor and may vary with each quarter. Can be repeated for credit. Variable credit.  PREREQUISITE(S): For specific prerequisites, see syllabus or consult course instructor. (variable credit)'},\n",
       " 'GEO441': {'Topic': 'Geographic Information Systems (Gis) for Community Development',\n",
       "  'Info': 'This course will focus on applications of Geographic Information Systems (GIS) to community studies and community development. As an amalgam of information technologies (e.g. database management, Web 2.0) and earth measurement technologies (e.g. global positioning systems, remote sensing), GIS is rapidly entering the realm of community development. The course will explain how GIS works; enable students to learn techniques including mapping, spatial analysis, and data management; and provide students with the opportunity to apply GIS to community development. Cross-listed with MPS 552.'},\n",
       " 'GEO442': {'Topic': 'Geographical Information Systems (Gis) for Sustainable Urban Development',\n",
       "  'Info': 'An intermediate-level GIS course focusing on the application of GIS skills to real-world problems. Students as a group will conduct GIS projects that are proposed by community-based organizations (CBOs) that work towards promoting sustainability in Chicagoland. Topics include remote sensing, GPS, geocoding, hot spot detection, spatial interpolation, and network analysis. The course also explores case studies on using GIS for sustainable urban development. PREREQUISITE(S): GEO 441 or MPS 552 (GIS for Community Development). GEO 441 or MPS 552 or instructor permission is a prerequisite for this course.'},\n",
       " 'GPH565': {'Topic': 'Designing for Visualization',\n",
       "  'Info': 'Sources of graphical integrity and sophistication.  Data-Ink maximization.  Data density. The use of color to enhance features in data sets and the communication of information .  Effective use of space and time.  Use of 3D techniques to display multi-dimensional data.  The use of isosurfaces and volumetric techniques to display features of data sets.  Students will use a programmable system to produce their visualizations and will learn how to use procedural techniques to express graphical intent.  (Only one of GPH 570 and GPH 565 may be taken for credit) Prerequisite(s): GPH 448 and HCI 470'},\n",
       " 'HCI512': {'Topic': 'Information Visualization and Infographics',\n",
       "  'Info': 'Communicating information through visualizations. Students learn how to choose effective means to visualize data for (a) their intended audience(s) and (b) for the message they intend to communicate. Students practice creating and evaluating visualizations using a variety of tools and methods.'},\n",
       " 'IPD451': {'Topic': 'Big Data and NoSQL Program',\n",
       "  'Info': 'A 11-week certificate program covering popular NoSQL databases and how they fit with Big Data. This program requires a separate application for admission and $40 application fee. Please visit IPD.CDM.DEPAUL.EDU for information on how to enroll.'},\n",
       " 'IS549': {'Topic': 'Data Warehousing',\n",
       "  'Info': 'Introduction to data warehousing and the foundations of understanding the issues involved in building a successful data warehouse. Data warehouse development methodology and issues surrounding the planning of the data warehouse. Data quality and metadata in the data warehouse. Analysis, transformation and loading of data into a data warehouse. Development of the data architecture and physical design. Implementation and administration of the data warehouse.  PREREQUISTE(S): CSC 451 or CSC 453 or CSC 455'},\n",
       " 'IS550': {'Topic': 'Enterprise Data Management',\n",
       "  'Info': 'This course focuses on the technical concepts and managerial knowledge needed to define, integrate and govern centralized and distributed data for a wide range of application systems used at large, multinational corporations. Topics include data repository, data life cycle, DAMA-DMBOK, data stewardship, data asset valuation, enterprise data architecture, data modeling with meta-data, data security standards, master data, and data quality management.'},\n",
       " 'IS574': {'Topic': 'Business Intelligence and Analytics Systems',\n",
       "  'Info': 'Introduction to the systems concept of business intelligence and analytics as components of a portfolio of information systems. How business problems can be solved strategically by implementing a portfolio of business intelligence and analytics systems including data lake, data warehouse, data discovery and visualization tools to gain new insights into organizational operations. Detailed discussions of the analysis, design, and implementation of business intelligence and analytics systems, including infrastructure for data management, portfolio management of technology life cycle, and alignment between corporate and local-unit needs. Case studies of application software, cloud resources, change management, as well as technical and social issues.'},\n",
       " 'MGT559': {'Topic': 'Health Sector Management',\n",
       "  'Info': \"This graduate level, hands-on course will discuss the evolution and current trends in the delivery and financing of health goods and services in the biotechnology, pharmaceutical, medical device, and health services delivery industries within the health sector. This course will equip students with the ability to use managerial epidemiology as a decision- making tool in marketing and operations in the health sector. Ultimately, this course will enable students to apply Michael Porter's Five Forces Model to analyze and manage the various industries within the health sector. This course will use lectures, role plays, simulations, and the case method.\"},\n",
       " 'MKT555': {'Topic': 'Marketing Management',\n",
       "  'Info': \"Students are provided with an overview of the marketing process for consumer-oriented firms.  Focus is placed on decision-making that aligns a firm's market offerings with the wants and needs of targeted segments of customers within a continuously changing environment.  Written cases/projects are part of the course assignment.\"},\n",
       " 'MKT530': {'Topic': 'Customer Relationship Management',\n",
       "  'Info': 'Students are introduced to a new strategic methodology, Customer Relationship Management, which is currently being adopted by many organizations in efforts to enhance their competitive advantage. Focus is placed on understanding how enhanced customer relationships will differentiate an organization in a highly competitive marketplace.  Both business and consumer markets are examined and discussed during this highly interactive class. Relevant guest speakers will share their experiences and current best practices. Case analysis and group projects supplement the class discussions. MKT 555 is a prerequisite for this course. MKT 555 is a prerequisite for this class.'},\n",
       " 'MKT534': {'Topic': 'Analytical Tools for Marketers',\n",
       "  'Info': 'This course seeks to provide an in-depth understanding of both qualitative and quantitative analytical tools that are of critical importance to marketers.  These tools will help marketers avoid head-to-head competition, understand customer perceptions, understand customer preferences, develop accurate sales forecasts, and financially value marketing strategies.  The course is designed to be \"hands-on\" in that students will develop understanding mainly through conducting application projects and presenting results.  The course is also designed to be immediately applicable to marketers\\' current and future jobs. MKT 555 is a Prerequisite for this course MKT 555 is a prerequisite for this class.'},\n",
       " 'MKT595': {'Topic': 'Digital Marketing Analytics and Planning',\n",
       "  'Info': 'Explores the emerging business models, rules, tactics, and strategies associated with this medium.  Integration with other channels and marketing operations is stressed.  Classes are discussion-based, drawing on current applied readings and cases from a variety of industries in both the business-to-business and business-consumer markets. MKT 555 is a Prerequisite for this course MKT 555 is a prerequisite for this class.'},\n",
       " 'CSC695': {'Topic': \"Master's Research\",\n",
       "  'Info': \"Students interested in a more in-depth study of a particular area will register for this course and work with a faculty member (not necessarily their academic advisor) on a research project. The work involved may include system development, empirical studies, or theoretical work. 4 credit hours of CSC 695 replaces one 500level elective course in student's degree program. This course can be taken for 1-4 credit hours for up 8 credit-hours.  PREREQUISITE(S): Consent of research advisor. Independent study form required. Students must successfully complete the foundation courses prior to their first enrollment in CSC 695. (variable credit )\"},\n",
       " 'DSC672': {'Topic': 'Data Science Capstone',\n",
       "  'Info': 'The capstone course provides an opportunity for students to integrate and apply the analytics skills and knowledge learned in the classroom to real world data. Students work in teams on a large scale analytics project. At the end of the course, students submit a report summarizing their analyses and study outcomes, and present results to the class. PREREQUISITE(S): Instructor consent required'},\n",
       " 'CSC697': {'Topic': 'Graduate Internship',\n",
       "  'Info': 'In cooperation with local employers, the graduate program offers students the opportunity to integrate their academic experience with on-the-job training in computer related work areas. This course is variable credit and may be taken for one to four credits. This course may be repeated for a maximum of four credits total. Admission to the internship program requires consent of the Instructor and a Student Services Advisor. International students may complete curricular practical training (CPT) through this class provided they first obtain CPT authorization from the Office for International Students and Scholars (OISS) before beginning the internship.  (1 quarter hour)'},\n",
       " 'MGT798': {'Topic': 'Special Topics',\n",
       "  'Info': 'Content and format of this course are variable.  An in-depth study of current issues in management.'},\n",
       " 'MKT570': {'Topic': 'Service Design and Patient Experience',\n",
       "  'Info': \"The primary objective of this course is to provide students with in-depth knowledge of how patient experience is impacting health care businesses today, including relationships among patient experience, clinical measures, and operational success. Students will understand the various components of patient experience, identify appropriate metrics for assessing patient experience (CAHPS data, Press Ganey data, Customer Lifetime Value, etc.), and learn to utilize tools for assessing and (re)designing service processes. Working in teams that will be mentored by working professionals in health care, students will develop projects that propose interventions to innovate and improve patient experience metrics. This course will serve as a link between master's level coursework and internships/jobs in health care management, marketing and patient experience. MKT 555 is a prerequisite for this class.\"},\n",
       " 'MKT798': {'Topic': 'Special Topics',\n",
       "  'Info': 'Content and format of this course is variable.  An in-depth study of current issues in marketing.'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2786a53",
   "metadata": {},
   "source": [
    "Above is the output of the courses dictionary before linguistic processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f7cdcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'Course ID': 'IT 403',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=IT&CatalogNbr=403'},\n",
       " 1: {'Course ID': 'CSC 412',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=412'},\n",
       " 2: {'Course ID': 'CSC 401',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=401'},\n",
       " 3: {'Course ID': 'DSC 450',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=450'},\n",
       " 4: {'Course ID': 'DSC 423',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=423'},\n",
       " 5: {'Course ID': 'DSC 424',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=424'},\n",
       " 6: {'Course ID': 'DSC 430',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=430'},\n",
       " 7: {'Course ID': 'DSC 441',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=441'},\n",
       " 8: {'Course ID': 'DSC 465',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=465'},\n",
       " 9: {'Course ID': 'DSC 480',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=480'},\n",
       " 10: {'Course ID': 'DSC 478',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=478'},\n",
       " 11: {'Course ID': 'CSC 555',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=555'},\n",
       " 12: {'Course ID': 'DSC 540',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=540'},\n",
       " 13: {'Course ID': 'CSC 521',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=521'},\n",
       " 14: {'Course ID': 'CSC 575',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=575'},\n",
       " 15: {'Course ID': 'CSC 578',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=578'},\n",
       " 16: {'Course ID': 'DSC 425',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=425'},\n",
       " 17: {'Course ID': 'DSC 433',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=433'},\n",
       " 18: {'Course ID': 'CSC 452',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=452'},\n",
       " 19: {'Course ID': 'CSC 481',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=481'},\n",
       " 20: {'Course ID': 'CSC 482',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=482'},\n",
       " 21: {'Course ID': 'DSC 484',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=484'},\n",
       " 22: {'Course ID': 'CSC 528',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=528'},\n",
       " 23: {'Course ID': 'DSC 510',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=510'},\n",
       " 24: {'Course ID': 'CSC 543',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=543'},\n",
       " 25: {'Course ID': 'CSC 576',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=576'},\n",
       " 26: {'Course ID': 'CSC 577',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=577'},\n",
       " 27: {'Course ID': 'CSC 594',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=594'},\n",
       " 28: {'Course ID': 'CSC 598',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=598'},\n",
       " 29: {'Course ID': 'GEO 441',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=GEO&CatalogNbr=441'},\n",
       " 30: {'Course ID': 'GEO 442',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=GEO&CatalogNbr=442'},\n",
       " 31: {'Course ID': 'GPH 565',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=GPH&CatalogNbr=565'},\n",
       " 32: {'Course ID': 'HCI 512',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=HCI&CatalogNbr=512'},\n",
       " 33: {'Course ID': 'IPD 451',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=IPD&CatalogNbr=451'},\n",
       " 34: {'Course ID': 'IS 549',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=IS&CatalogNbr=549'},\n",
       " 35: {'Course ID': 'IS 550',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=IS&CatalogNbr=550'},\n",
       " 36: {'Course ID': 'IS 574',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=IS&CatalogNbr=574'},\n",
       " 37: {'Course ID': 'MGT 559',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MGT&CatalogNbr=559'},\n",
       " 38: {'Course ID': 'MKT 555',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=555'},\n",
       " 39: {'Course ID': 'MKT 530',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=530'},\n",
       " 40: {'Course ID': 'MKT 534',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=534'},\n",
       " 41: {'Course ID': 'MKT 595',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=595'},\n",
       " 42: {'Course ID': 'CSC 695',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=695'},\n",
       " 43: {'Course ID': 'DSC 672',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=DSC&CatalogNbr=672'},\n",
       " 44: {'Course ID': 'CSC 697',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=CSC&CatalogNbr=697'},\n",
       " 45: {'Course ID': 'MGT 798',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MGT&CatalogNbr=798'},\n",
       " 46: {'Course ID': 'MKT 570',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=570'},\n",
       " 47: {'Course ID': 'MKT 798',\n",
       "  'URL': 'https://www.cdm.depaul.edu/academics/pages/courseinfo.aspx?Subject=MKT&CatalogNbr=798'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courseID_docID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a5779",
   "metadata": {},
   "source": [
    "Above is the document number and course ID mapping. This will be used to display results to the user. As seen above, there are 48 courses being used in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ea883",
   "metadata": {},
   "source": [
    "## Indexing <a id=Index>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41f08b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading courses json file from web crawler\n",
    "with open('courses.json', 'r') as infile:\n",
    "    courses = json.load(infile)\n",
    "\n",
    "##### Creating inverted index for Course Information \n",
    "info_inverted_index = []\n",
    "\n",
    "##### Tokenization, Stop Word Removal, and Stemming for Course Information \n",
    "tokens_info = [word_tokenize(courses[key]['Info']) for key in courses.keys()]  # Tokens for course info\n",
    "# Lowering each token, removing punctuation, removing stop words, and stemming\n",
    "port_stemmer = PorterStemmer()\n",
    "for doc_id, tokens_lst in enumerate(tokens_info):\n",
    "    cleaned_tokens = [token.lower() for token in tokens_lst if token.isalpha() and token not in stopwords.words('english') and len(token) > 1]\n",
    "    stemmed_tokens = [port_stemmer.stem(token) for token in cleaned_tokens]\n",
    "    for token in stemmed_tokens:\n",
    "        info_inverted_index.append([token, doc_id])\n",
    "\n",
    "info_inverted_index = sorted(info_inverted_index)  # Sorting the inverted index\n",
    "\n",
    "##### Creating index/dictionary and postings for course info\n",
    "info_index = {}\n",
    "info_postings = {}  # (docID, term frequency)\n",
    "doc_id_lst = []  # Keep track of doc_id's for postings list\n",
    "first_iteration = True \n",
    "count = 0\n",
    "prev_docID = None\n",
    "prev_term = None\n",
    "for term, docID in info_inverted_index:\n",
    "    if term not in info_index.keys():  # New term\n",
    "        info_index[term] = {'Doc Freq': 1, 'Total Freq': 1}\n",
    "        # Update postings list from prior\n",
    "        if first_iteration == False:\n",
    "            doc_id_lst.append([prev_docID, count])\n",
    "            count = 0\n",
    "            info_postings[prev_term] = doc_id_lst\n",
    "            doc_id_lst = []\n",
    "    else:  # Already added term\n",
    "        if docID == prev_docID:  # Term is within same document\n",
    "            info_index[term]['Total Freq'] += 1\n",
    "        else:  # Term is in a new document\n",
    "            info_index[term]['Doc Freq'] += 1\n",
    "            info_index[term]['Total Freq'] += 1\n",
    "            doc_id_lst.append((prev_docID, count))\n",
    "            count = 0 \n",
    "    count += 1\n",
    "    prev_docID = docID  # Assigning docID as previous docID\n",
    "    prev_term = term  # Assigning term as previous term\n",
    "    first_iteration = False\n",
    "# Adding last postings term \n",
    "doc_id_lst.append((prev_docID, count))\n",
    "info_postings[prev_term] = doc_id_lst \n",
    "    \n",
    "\n",
    "##### Saving info index and info postings as json files\n",
    "with open('info_index.json', 'w') as outfile1:\n",
    "    json.dump(info_index, outfile1)\n",
    "\n",
    "with open('info_postings.json', 'w') as outfile2:\n",
    "    json.dump(info_postings, outfile2)\n",
    "    \n",
    "    \n",
    "##### Creating inverted index for Course Topic\n",
    "topic_inverted_index = []\n",
    "\n",
    "##### Tokenization, Stop Word Removal, and Stemming for Course Topic\n",
    "tokens_info = [word_tokenize(courses[key]['Topic']) for key in courses.keys()]  # Tokens for course info\n",
    "# Lowering each token, removing punctuation, removing stop words, and stemming\n",
    "port_stemmer = PorterStemmer()\n",
    "for doc_id, tokens_lst in enumerate(tokens_info):\n",
    "    cleaned_tokens = [token.lower() for token in tokens_lst if token.isalpha() and token not in stopwords.words('english') and len(token) > 1]\n",
    "    stemmed_tokens = [port_stemmer.stem(token) for token in cleaned_tokens]\n",
    "    for token in stemmed_tokens:\n",
    "        topic_inverted_index.append([token, doc_id])\n",
    "\n",
    "topic_inverted_index = sorted(topic_inverted_index)  # Sorting the inverted index\n",
    "\n",
    "##### Creating index/dictionary and postings for course topic\n",
    "topic_index = {}\n",
    "topic_postings = {}  # (docID, term frequency)\n",
    "doc_id_lst = []  # Keep track of doc_id's for postings list\n",
    "first_iteration = True \n",
    "count = 0\n",
    "for term, docID in topic_inverted_index:\n",
    "    if term not in topic_index.keys():  # New term\n",
    "        topic_index[term] = {'Doc Freq': 1, 'Total Freq': 1}\n",
    "        # Update postings list from prior\n",
    "        if first_iteration == False:\n",
    "            doc_id_lst.append([prev_docID, count])\n",
    "            count = 0\n",
    "            topic_postings[prev_term] = doc_id_lst\n",
    "            doc_id_lst = []\n",
    "    else:  # Already added term\n",
    "        if docID == prev_docID:  # Term is within same document\n",
    "            topic_index[term]['Total Freq'] += 1\n",
    "        else:  # Term is in a new document\n",
    "            topic_index[term]['Doc Freq'] += 1\n",
    "            topic_index[term]['Total Freq'] += 1\n",
    "            doc_id_lst.append((prev_docID, count))\n",
    "            count = 0 \n",
    "    count += 1\n",
    "    prev_docID = docID  # Assigning docID as previous docID\n",
    "    prev_term = term  # Assigning term as previous term\n",
    "    first_iteration = False\n",
    "    \n",
    "# Adding last postings term \n",
    "doc_id_lst.append((prev_docID, count))\n",
    "topic_postings[prev_term] = doc_id_lst \n",
    "\n",
    "# Saving topic index, topic postings, \n",
    "with open('topic_index.json', 'w') as outfile3:\n",
    "    json.dump(topic_index, outfile3)\n",
    "\n",
    "with open('topic_postings.json', 'w') as outfile4:\n",
    "    json.dump(topic_postings, outfile4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fcb181",
   "metadata": {},
   "source": [
    "Above is the linguistic processing techniques that were applied to the bag of words. The terms were lower cased, stop words were removed, punctuation was removed, and a Porter Stemming algorithm was applied. An inverted index was created for the course topics and a separate inverted index was created for the course description. Each inverted index resulted in an index/dictionary and a postings list. All four of these files were saved to disk to be used by the information retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2b5d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advanc': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'advertis': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'algorithm': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'analysi': {'Doc Freq': 9, 'Total Freq': 9},\n",
       " 'analyt': {'Doc Freq': 4, 'Total Freq': 4},\n",
       " 'appli': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'applic': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'artifici': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'big': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'busi': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'capston': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'carlo': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'commun': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'comput': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'custom': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'data': {'Doc Freq': 14, 'Total Freq': 14},\n",
       " 'databas': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'deep': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'design': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'develop': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'digit': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'enterpris': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'experi': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'forecast': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'fundament': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'geograph': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'gi': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'graduat': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'health': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'imag': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'infograph': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'inform': {'Doc Freq': 5, 'Total Freq': 5},\n",
       " 'intellig': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'internship': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'introduct': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'learn': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'machin': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'manag': {'Doc Freq': 4, 'Total Freq': 4},\n",
       " 'market': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'master': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'mine': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'mont': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'network': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'neural': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'nosql': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'patient': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'plan': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'process': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'program': {'Doc Freq': 5, 'Total Freq': 5},\n",
       " 'python': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'recommend': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'regress': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'relationship': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'research': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'retriev': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'scienc': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'script': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'sector': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'seri': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'servic': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'social': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'spatial': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'special': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'statist': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'sustain': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'system': {'Doc Freq': 5, 'Total Freq': 5},\n",
       " 'techniqu': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'time': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'tool': {'Doc Freq': 2, 'Total Freq': 2},\n",
       " 'topic': {'Doc Freq': 4, 'Total Freq': 4},\n",
       " 'urban': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'vision': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'visual': {'Doc Freq': 3, 'Total Freq': 3},\n",
       " 'wareh': {'Doc Freq': 1, 'Total Freq': 1},\n",
       " 'web': {'Doc Freq': 1, 'Total Freq': 1}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6d441e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'advanc': [(5, 1), [12, 1]],\n",
       " 'advertis': [[25, 1]],\n",
       " 'algorithm': [[13, 1]],\n",
       " 'analysi': [(0, 1),\n",
       "  (1, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (9, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (20, 1),\n",
       "  [28, 1]],\n",
       " 'analyt': [(3, 1), (36, 1), (40, 1), [41, 1]],\n",
       " 'appli': [[20, 1]],\n",
       " 'applic': [[10, 1]],\n",
       " 'artifici': [[27, 1]],\n",
       " 'big': [(11, 1), [33, 1]],\n",
       " 'busi': [[36, 1]],\n",
       " 'capston': [[43, 1]],\n",
       " 'carlo': [[13, 1]],\n",
       " 'commun': [[29, 1]],\n",
       " 'comput': [(1, 1), (22, 1), [25, 1]],\n",
       " 'custom': [[39, 1]],\n",
       " 'data': [(0, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (11, 1),\n",
       "  (17, 1),\n",
       "  (21, 1),\n",
       "  (23, 1),\n",
       "  (28, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  [43, 1]],\n",
       " 'databas': [(3, 1), (18, 1), [24, 1]],\n",
       " 'deep': [[15, 1]],\n",
       " 'design': [(31, 1), [46, 1]],\n",
       " 'develop': [(29, 1), [30, 1]],\n",
       " 'digit': [[41, 1]],\n",
       " 'enterpris': [[35, 1]],\n",
       " 'experi': [[46, 1]],\n",
       " 'forecast': [[16, 1]],\n",
       " 'fundament': [[7, 1]],\n",
       " 'geograph': [(24, 1), (29, 1), [30, 1]],\n",
       " 'gi': [(29, 1), [30, 1]],\n",
       " 'graduat': [[44, 1]],\n",
       " 'health': [(23, 1), [37, 1]],\n",
       " 'imag': [(19, 1), [20, 1]],\n",
       " 'infograph': [[32, 1]],\n",
       " 'inform': [(14, 1), (24, 1), (29, 1), (30, 1), [32, 1]],\n",
       " 'intellig': [(14, 1), (27, 1), [36, 1]],\n",
       " 'internship': [[44, 1]],\n",
       " 'introduct': [(2, 1), [19, 1]],\n",
       " 'learn': [(10, 1), (12, 1), [15, 1]],\n",
       " 'machin': [(10, 1), [12, 1]],\n",
       " 'manag': [(35, 1), (37, 1), (38, 1), [39, 1]],\n",
       " 'market': [(38, 1), (40, 1), [41, 1]],\n",
       " 'master': [[42, 1]],\n",
       " 'mine': [(11, 1), [21, 1]],\n",
       " 'mont': [[13, 1]],\n",
       " 'network': [(9, 1), [15, 1]],\n",
       " 'neural': [[15, 1]],\n",
       " 'nosql': [[33, 1]],\n",
       " 'patient': [[46, 1]],\n",
       " 'plan': [[41, 1]],\n",
       " 'process': [(3, 1), [19, 1]],\n",
       " 'program': [(2, 1), (6, 1), (10, 1), (18, 1), [33, 1]],\n",
       " 'python': [[6, 1]],\n",
       " 'recommend': [[26, 1]],\n",
       " 'regress': [[4, 1]],\n",
       " 'relationship': [[39, 1]],\n",
       " 'research': [[42, 1]],\n",
       " 'retriev': [[14, 1]],\n",
       " 'scienc': [(7, 1), (23, 1), [43, 1]],\n",
       " 'script': [[17, 1]],\n",
       " 'sector': [[37, 1]],\n",
       " 'seri': [[16, 1]],\n",
       " 'servic': [[46, 1]],\n",
       " 'social': [[9, 1]],\n",
       " 'spatial': [[24, 1]],\n",
       " 'special': [(45, 1), [47, 1]],\n",
       " 'statist': [[0, 1]],\n",
       " 'sustain': [[30, 1]],\n",
       " 'system': [(24, 1), (26, 1), (29, 1), (30, 1), [36, 1]],\n",
       " 'techniqu': [[1, 1]],\n",
       " 'time': [[16, 1]],\n",
       " 'tool': [(1, 1), [40, 1]],\n",
       " 'topic': [(27, 1), (28, 1), (45, 1), [47, 1]],\n",
       " 'urban': [[30, 1]],\n",
       " 'vision': [[22, 1]],\n",
       " 'visual': [(8, 1), (31, 1), [32, 1]],\n",
       " 'wareh': [[34, 1]],\n",
       " 'web': [(21, 1)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f676e153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in course description: 682\n",
      "Terms in course title: 75\n"
     ]
    }
   ],
   "source": [
    "print('Terms in course description: {}'.format(len(info_index)))\n",
    "print('Terms in course title: {}'.format(len(topic_postings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd471076",
   "metadata": {},
   "source": [
    "## Information Retrieval System <a id=Retrieval>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cea0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading info index, info postings, and docID_mapping from indexing\n",
    "with open('info_index.json', 'r') as infile1:\n",
    "    info_index = json.load(infile1)\n",
    "    \n",
    "with open('info_postings.json', 'r') as infile2:\n",
    "    info_postings = json.load(infile2)\n",
    "\n",
    "with open('docID_mapping.json', 'r') as infile3:\n",
    "    docID_mapping = json.load(infile3)\n",
    "\n",
    "# Loading topic index, and topic postings\n",
    "with open('topic_index.json', 'r') as infile4:\n",
    "    topic_index = json.load(infile4)\n",
    "    \n",
    "with open('topic_postings.json', 'r') as infile5:\n",
    "    topic_postings = json.load(infile5)\n",
    "\n",
    "class QueryRetrieval:\n",
    "    def __init__(self, info_index, info_postings, topic_index, topic_postings, mapping, k, d, topic_weight):\n",
    "        # Info is the course description while topic is the course name \n",
    "        self.info_index = info_index  \n",
    "        self.info_postings = info_postings\n",
    "        self.topic_index = topic_index\n",
    "        self.topic_postings = topic_postings\n",
    "        self.mapping = mapping \n",
    "        self.N = len(mapping)\n",
    "        self.k = k  # Number of retrieved docs based on cosine similarity \n",
    "        self.d = d  # Top d documents returned to user \n",
    "        self.topic_weight = topic_weight  # Weight of the topic name keywords similarity (Course info weight is 1-topic_weight)\n",
    "    \n",
    "    def RetrieveDocs(self, query):\n",
    "        \"\"\"Retrieves top 3 Doc URL's based on User Query\"\"\"\n",
    "        self.query = str(query)\n",
    "        # Obtaining IDF Weights\n",
    "        info_idf_weights = self.IDF(info_index) \n",
    "        topic_idf_weights = self.IDF(self.topic_index) \n",
    "        # Creating vector space model of postings matrix\n",
    "        info_posting_matrix = self.VectorSpace(self.info_postings)\n",
    "        topic_posting_matrix = self.VectorSpace(self.topic_postings) \n",
    "        # Normalizing vector space model\n",
    "        normalized_info_matrix = self.TFIDF(info_idf_weights, info_posting_matrix)\n",
    "        normalized_topic_matrix = self.TFIDF(topic_idf_weights, topic_posting_matrix)\n",
    "        # Tokenizing query and linguistic processing \n",
    "        tokenized_query = self.QueryTokens()\n",
    "        # Vector representation of the query\n",
    "        info_vector_query = self.QueryVector(tokenized_query, self.info_postings)\n",
    "        topic_vector_query = self.QueryVector(tokenized_query, self.topic_postings)\n",
    "        # Calculating Similarity Values \n",
    "        info_sim_list = self.CosineSimilarity(normalized_info_matrix, info_vector_query)\n",
    "        topic_sim_list = self.CosineSimilarity(normalized_topic_matrix, topic_vector_query)\n",
    "        # Computing Weighted Similarity Value \n",
    "        query_docs = self.DocRetrieval(info_sim_list, topic_sim_list)\n",
    "        # Opens top d document URLs to user\n",
    "        for doc_id, sim in query_docs:\n",
    "            course_URL = self.mapping[str(doc_id)]['URL']\n",
    "            webbrowser.open(course_URL)\n",
    "    \n",
    "    def QueryTokens(self):\n",
    "        \"\"\"Tokenizes the Query Based on the Rules for the Document Index and Posting Tokenization\"\"\"\n",
    "        modified_query = []\n",
    "        tokens_query = word_tokenize(self.query)  # Tokens of the query\n",
    "        modified_query = []\n",
    "        port_stemmer = PorterStemmer()\n",
    "        for token in tokens_query:\n",
    "            if token.isalpha() and token not in stopwords.words('english'):  # Removing stop words and punctuation \n",
    "                modified_query.append(port_stemmer.stem(token))  # Stemming the tokens\n",
    "        \n",
    "        return modified_query\n",
    "    \n",
    "    def QueryVector(self, token_query, postings):\n",
    "        \"\"\"Returns a Vector Representation of the Query Using the Postings List\"\"\"\n",
    "        vector = np.zeros(len(postings))\n",
    "        for term in token_query:  # Iterating through each query term\n",
    "            for index, key in enumerate(postings.keys()):  # Iterating through the term dictionary, aka the keys\n",
    "                if term == key:  # Finding when the query term matches the dictionary\n",
    "                    vector[index] += 1  # Adding a one to the vector location\n",
    "        return vector\n",
    "        \n",
    "    def IDF(self, index):\n",
    "        \"\"\"Returns IDF Weights for the Index\"\"\"\n",
    "        idf_lst = []\n",
    "        for term in index.keys():\n",
    "            idf_weight = np.log2(self.N/index[term]['Doc Freq']) # using log2\n",
    "            idf_lst.append(idf_weight)\n",
    "        return idf_lst            \n",
    "\n",
    "    def TFIDF(self, idf_vals, non_normalized_matrix):\n",
    "        \"\"\"Calculates the normalized postings list using TF*IDF Weights\"\"\"\n",
    "        non_normalized_matrix = np.transpose(non_normalized_matrix)\n",
    "        normalized_matrix = []\n",
    "        for row in range(len(idf_vals)):\n",
    "            new_val = np.round(idf_vals[row] * non_normalized_matrix[row], 3) # IDF * TF for each term as an array\n",
    "            normalized_val = np.round(np.sqrt(sum(idf_vals[row]**2 * non_normalized_matrix[row]**2)),3)  # Normalizing the value \n",
    "            normalized_matrix.append(new_val/normalized_val)\n",
    "        normalized_matrix = np.transpose(normalized_matrix)\n",
    "    \n",
    "        return normalized_matrix\n",
    "     \n",
    "    def VectorSpace(self, postings):\n",
    "        \"\"\"Vector Space Representation of the Postings List\"\"\"\n",
    "        # Sparse Empty Matrix\n",
    "        df = pd.DataFrame(columns = postings.keys(), index=list(range(self.N)))\n",
    "        df.fillna(value=0, inplace=True)  # Creating sparse matrix of zeros\n",
    "        \n",
    "        for term in postings.keys():  # Iterating through each term in postings list (key)\n",
    "            for occurence in postings[term]:  # Obtaining docID and freq for each term\n",
    "                doc_num = occurence[0]\n",
    "                term_freq = occurence[1]\n",
    "                df.at[doc_num, term] += term_freq  # Updating term frequency \n",
    "\n",
    "        return np.array(df)  # Returning numpy array of df\n",
    "\n",
    "    def CosineSimilarity(self, posting_matrix, vector_query):\n",
    "        \"\"\"Calculates the Cosine Similarity between each Document Vector and the Query Vector. Returns Top K Similar Documents\"\"\"\n",
    "        cosine_sim_list = []\n",
    "        for docID, doc in enumerate(posting_matrix):\n",
    "            dot_prod = np.dot(doc, vector_query)\n",
    "            x_norm = 0\n",
    "            y_norm = 0\n",
    "            \n",
    "            for val in doc:\n",
    "                x_norm += val**2\n",
    "            \n",
    "            for val in vector_query:\n",
    "                y_norm += val**2\n",
    "            \n",
    "            if x_norm * y_norm != 0:  # Catch dot prod zero errors where no terms appear\n",
    "                cosine_similarity_val = round(dot_prod/(np.sqrt(x_norm * y_norm)), 3)        \n",
    "                cosine_sim_list.append([cosine_similarity_val, docID])\n",
    "    \n",
    "        sorted_list = sorted(cosine_sim_list, reverse=True)\n",
    "        return sorted_list[0:self.k]  \n",
    "\n",
    "    def DocRetrieval(self, info_similarity, topic_similarity):\n",
    "        \"\"\"Returns the Top D Courses to the Items Based on the Weighted Similarity Measure between Topic and Info of the course\"\"\"\n",
    "        info_weight = 1 - self.topic_weight\n",
    "        topic_weight = self.topic_weight\n",
    "        \n",
    "        combined_sim_dic = {}\n",
    "        \n",
    "        for similarity in info_similarity: \n",
    "            doc_id = similarity[1]\n",
    "            sim_val = similarity[0] * info_weight\n",
    "            combined_sim_dic[doc_id] = sim_val\n",
    "            \n",
    "        for similarity in topic_similarity:  \n",
    "            doc_id = similarity[1]\n",
    "            sim_val = similarity[0] * topic_weight\n",
    "            if doc_id not in combined_sim_dic.keys():\n",
    "                combined_sim_dic[doc_id] = sim_val\n",
    "            else:\n",
    "                combined_sim_dic[doc_id] += sim_val\n",
    "        \n",
    "        return list(sorted(combined_sim_dic.items(), key=lambda item: item[1], reverse=True))[0:self.d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503277c8",
   "metadata": {},
   "source": [
    "Above is the code for the information retrieval system. The information retrieval system takes a query and uses the same linguistic preprocessing techniques. Using the index and postings list a vector space model is created for both the course description and course title. Both of these separate vector spaces use a normalized TFxIDF weighting. The query is also represented in a vector space. This allow for Cosine Similarity to be measured between the query and each document. Each vector space model returns k number of similar documents. The weighted cosine score is calculated based on the course topic weight. Afterwards d number of documents are returned to the user by opening the course webpage in the web browser of the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efde598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10  # Number of Similar Documents Returned to Weighting System\n",
    "d = 3  # Number of Documents Displayed to User\n",
    "topic_weight = .2  # Weight for the course name (Course description weight is 1-topic_weight)\n",
    "x = QueryRetrieval(info_index, info_postings, topic_index, topic_postings, docID_mapping, k, d, topic_weight)      \n",
    "x.RetrieveDocs('Regression SQL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738f9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.RetrieveDocs('Regressions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f43d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.RetrieveDocs('Mining')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d830d",
   "metadata": {},
   "source": [
    "As seen above by these three trials it helps eliminate the three limitations as noted above in the introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727116f2",
   "metadata": {},
   "source": [
    "## Test  Case <a id=Test>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aceafba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MedLine Collection http://ir.dcs.gla.ac.uk/resources/test_collections/medl/\n",
    "### Chosen because there is an article title and description \n",
    "#### Reading in the files\n",
    "# Documents\n",
    "raw_data = {}  # Will hold the raw form of each document\n",
    "doc_id = 0\n",
    "first_iteration = True\n",
    "with open('./ML/MED.ALL') as infile:\n",
    "    new_doc = []\n",
    "    x = infile.readlines()\n",
    "    for line in x:\n",
    "        txt = line.strip()\n",
    "        first_two = txt[:2]\n",
    "        if first_two == '.I':  # Marks a new document\n",
    "            if first_iteration == False:\n",
    "                raw_data[doc_id] = ' '.join(new_doc)\n",
    "                new_doc = []\n",
    "                doc_id += 1\n",
    "            else:\n",
    "                first_iteration = False\n",
    "                doc_id += 1\n",
    "        elif len(txt) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            new_doc.append(txt)\n",
    "            \n",
    "    raw_data[doc_id] = ' '.join(new_doc)\n",
    "\n",
    "# Creating a Topic Dictionary (Article title)\n",
    "# Creating a Info Dictionary (Article description)\n",
    "topic_dict = {}  # Will hold the article titles for each document\n",
    "info_dict = {}  # Will hold the article description for each document \n",
    "for key in raw_data.keys():\n",
    "    description = []\n",
    "    val = raw_data[key]\n",
    "    sentences = val.split('.')\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if index == 0:  # First sentence is the article topic\n",
    "            topic_dict[key] = sentence\n",
    "        else:  # All other sentences are the article description \n",
    "            description += [sentence]\n",
    "    info_dict[key] = ' '.join(description)\n",
    "    \n",
    "\n",
    "# Queries\n",
    "raw_queries = {}  # Will hold each query \n",
    "query_id = 0\n",
    "first_iteration = True\n",
    "with open('./ML/MED.QRY') as infile:\n",
    "    new_query = []\n",
    "    x = infile.readlines()\n",
    "    for line in x:\n",
    "        txt = line.strip()\n",
    "        if len(txt) == 4 or len(txt) == 5:\n",
    "            if first_iteration == False:\n",
    "                raw_queries[query_id] = ' '.join(new_query)\n",
    "                new_query = []\n",
    "                query_id += 1\n",
    "            else:\n",
    "                first_iteration = False\n",
    "                query_id += 1\n",
    "        elif len(txt) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            new_query.append(txt)\n",
    "            \n",
    "    raw_queries[query_id] = ' '.join(new_query)\n",
    "\n",
    "### Tokenization according to project model (Using the same process that was peformed for the course catalog project)\n",
    "### Only changed the lines with tokens_info variable (2 lines) to get each dictionary accordingly to new name \n",
    "\n",
    "##### Creating inverted index for Course Information \n",
    "info_inverted_index = []\n",
    "\n",
    "##### Tokenization, Stop Word Removal, and Stemming for Article Description\n",
    "tokens_info = [word_tokenize(info_dict[key]) for key in info_dict.keys()]  # Tokens for Article Description\n",
    "# Lowering each token, removing punctuation, removing stop words, and stemming\n",
    "port_stemmer = PorterStemmer()\n",
    "for doc_id, tokens_lst in enumerate(tokens_info):\n",
    "    cleaned_tokens = [token.lower() for token in tokens_lst if token.isalpha() and token not in stopwords.words('english') and len(token) > 1]\n",
    "    stemmed_tokens = [port_stemmer.stem(token) for token in cleaned_tokens]\n",
    "    for token in stemmed_tokens:\n",
    "        info_inverted_index.append([token, doc_id])\n",
    "\n",
    "info_inverted_index = sorted(info_inverted_index)  # Sorting the inverted index\n",
    "\n",
    "##### Creating index/dictionary and postings for course description \n",
    "info_index = {}\n",
    "info_postings = {}  # (docID, term frequency)\n",
    "doc_id_lst = []  # Keep track of doc_id's for postings list\n",
    "first_iteration = True \n",
    "count = 0\n",
    "prev_docID = None\n",
    "prev_term = None\n",
    "for term, docID in info_inverted_index:\n",
    "    if term not in info_index.keys():  # New term\n",
    "        info_index[term] = {'Doc Freq': 1, 'Total Freq': 1}\n",
    "        # Update postings list from prior\n",
    "        if first_iteration == False:\n",
    "            doc_id_lst.append([prev_docID, count])\n",
    "            count = 0\n",
    "            info_postings[prev_term] = doc_id_lst\n",
    "            doc_id_lst = []\n",
    "    else:  # Already added term\n",
    "        if docID == prev_docID:  # Term is within same document\n",
    "            info_index[term]['Total Freq'] += 1\n",
    "        else:  # Term is in a new document\n",
    "            info_index[term]['Doc Freq'] += 1\n",
    "            info_index[term]['Total Freq'] += 1\n",
    "            doc_id_lst.append((prev_docID, count))\n",
    "            count = 0 \n",
    "    count += 1\n",
    "    prev_docID = docID  # Assigning docID as previous docID\n",
    "    prev_term = term  # Assigning term as previous term\n",
    "    first_iteration = False\n",
    "# Adding last postings term \n",
    "doc_id_lst.append((prev_docID, count))\n",
    "info_postings[prev_term] = doc_id_lst     \n",
    "    \n",
    "##### Creating inverted index for Article Topic\n",
    "topic_inverted_index = []\n",
    "\n",
    "##### Tokenization, Stop Word Removal, and Stemming for Course Topic\n",
    "tokens_info = [word_tokenize(topic_dict[key]) for key in topic_dict.keys()]  # Tokens for course info\n",
    "# Lowering each token, removing punctuation, removing stop words, and stemming\n",
    "port_stemmer = PorterStemmer()\n",
    "for doc_id, tokens_lst in enumerate(tokens_info):\n",
    "    cleaned_tokens = [token.lower() for token in tokens_lst if token.isalpha() and token not in stopwords.words('english') and len(token) > 1]\n",
    "    stemmed_tokens = [port_stemmer.stem(token) for token in cleaned_tokens]\n",
    "    for token in stemmed_tokens:\n",
    "        topic_inverted_index.append([token, doc_id])\n",
    "\n",
    "topic_inverted_index = sorted(topic_inverted_index)  # Sorting the inverted index\n",
    "\n",
    "##### Creating index/dictionary and postings for course topic\n",
    "topic_index = {}\n",
    "topic_postings = {}  # (docID, term frequency)\n",
    "doc_id_lst = []  # Keep track of doc_id's for postings list\n",
    "first_iteration = True \n",
    "count = 0\n",
    "for term, docID in topic_inverted_index:\n",
    "    if term not in topic_index.keys():  # New term\n",
    "        topic_index[term] = {'Doc Freq': 1, 'Total Freq': 1}\n",
    "        # Update postings list from prior\n",
    "        if first_iteration == False:\n",
    "            doc_id_lst.append([prev_docID, count])\n",
    "            count = 0\n",
    "            topic_postings[prev_term] = doc_id_lst\n",
    "            doc_id_lst = []\n",
    "    else:  # Already added term\n",
    "        if docID == prev_docID:  # Term is within same document\n",
    "            topic_index[term]['Total Freq'] += 1\n",
    "        else:  # Term is in a new document\n",
    "            topic_index[term]['Doc Freq'] += 1\n",
    "            topic_index[term]['Total Freq'] += 1\n",
    "            doc_id_lst.append((prev_docID, count))\n",
    "            count = 0 \n",
    "    count += 1\n",
    "    prev_docID = docID  # Assigning docID as previous docID\n",
    "    prev_term = term  # Assigning term as previous term\n",
    "    first_iteration = False\n",
    "    \n",
    "# Adding last postings term \n",
    "doc_id_lst.append((prev_docID, count))\n",
    "topic_postings[prev_term] = doc_id_lst \n",
    "\n",
    "##### RECAP\n",
    "# Have the following 4 dictionaries \n",
    "## Two Indexes: info_index, topic_index\n",
    "## Two Postings: info_postings, topic_postings\n",
    "\n",
    "\n",
    "##### Saving info index and info postings as json files\n",
    "with open('test_info_index.json', 'w') as outfile1:\n",
    "    json.dump(info_index, outfile1)\n",
    "\n",
    "with open('test_info_postings.json', 'w') as outfile2:\n",
    "    json.dump(info_postings, outfile2)\n",
    "\n",
    "# Saving topic index, topic postings, \n",
    "with open('test_topic_index.json', 'w') as outfile3:\n",
    "    json.dump(topic_index, outfile3)\n",
    "\n",
    "with open('test_topic_postings.json', 'w') as outfile4:\n",
    "    json.dump(topic_postings, outfile4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f033e284",
   "metadata": {},
   "source": [
    "Above is the code used to test my information retrieval system using a collection of documents from MedLine. The MedLine corpus has 1,003 documents and 30 test queries alongside with it's relevant documents. This test corpus was used since it contains the article's title and article's description for all 1,003 documents. This allowed me to test different weighting schemes to see which one might work well with my information retrieval system. Above the documents went through the same pre-processing schemes as above. Also, two inverted indexes were also created each resulting in it's own index/dictionary and postings list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8756440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms in article description: 7669\n",
      "Terms in article title: 2534\n"
     ]
    }
   ],
   "source": [
    "print('Terms in article description: {}'.format(len(info_index)))\n",
    "print('Terms in article title: {}'.format(len(topic_postings)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ccb7e",
   "metadata": {},
   "source": [
    "In comparison to my information retrieval system, this corpus has a significantly larger number of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0158a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries\n",
    "raw_queries = {}  # Will hold each query \n",
    "query_id = 0\n",
    "first_iteration = True\n",
    "with open('./ML/MED.QRY') as infile:\n",
    "    new_query = []\n",
    "    x = infile.readlines()\n",
    "    for line in x:\n",
    "        txt = line.strip()\n",
    "        if len(txt) == 4 or len(txt) == 5:\n",
    "            if first_iteration == False:\n",
    "                raw_queries[query_id] = ' '.join(new_query)\n",
    "                new_query = []\n",
    "                query_id += 1\n",
    "            else:\n",
    "                first_iteration = False\n",
    "                query_id += 1\n",
    "        elif len(txt) == 2:\n",
    "            pass\n",
    "        else:\n",
    "            new_query.append(txt)\n",
    "            \n",
    "    raw_queries[query_id] = ' '.join(new_query)\n",
    "\n",
    "\n",
    "# Loading info index, info postings, \n",
    "with open('test_info_index.json', 'r') as infile1:\n",
    "    info_index = json.load(infile1)\n",
    "    \n",
    "with open('test_info_postings.json', 'r') as infile2:\n",
    "    info_postings = json.load(infile2)\n",
    "\n",
    "# Loading topic index, and topic postings\n",
    "with open('test_topic_index.json', 'r') as infile3:\n",
    "    topic_index = json.load(infile3)\n",
    "    \n",
    "with open('test_topic_postings.json', 'r') as infile4:\n",
    "    topic_postings = json.load(infile4)\n",
    "\n",
    "\n",
    "### Testing the Information Retrieval System\n",
    "\n",
    "class TestQueryRetrieval:\n",
    "    def __init__(self, info_index, info_postings, topic_index, topic_postings, k, d, topic_weight):\n",
    "        # Info is the course description while topic is the course name \n",
    "        self.info_index = info_index  \n",
    "        self.info_postings = info_postings\n",
    "        self.topic_index = topic_index\n",
    "        self.topic_postings = topic_postings\n",
    "        self.N = 1033  # Number of docs \n",
    "        self.k = k  # Number of retrieved docs based on cosine similarity \n",
    "        self.d = d  # Top d documents returned to user \n",
    "        self.topic_weight = topic_weight  # Weight of the topic name keywords similarity (Course info weight is 1-topic_weight)\n",
    "    \n",
    "    def RetrieveDocs(self, query):\n",
    "        \"\"\"Retrieves top 3 Doc URL's based on User Query\"\"\"\n",
    "        self.query = str(query)\n",
    "        # Obtaining IDF Weights\n",
    "        info_idf_weights = self.IDF(info_index) \n",
    "        topic_idf_weights = self.IDF(self.topic_index) \n",
    "        # Creating vector space model of postings matrix\n",
    "        info_posting_matrix = self.VectorSpace(self.info_postings)\n",
    "        topic_posting_matrix = self.VectorSpace(self.topic_postings) \n",
    "        # Normalizing vector space model\n",
    "        normalized_info_matrix = self.TFIDF(info_idf_weights, info_posting_matrix)\n",
    "        normalized_topic_matrix = self.TFIDF(topic_idf_weights, topic_posting_matrix)\n",
    "        # Tokenizing query and linguistic processing \n",
    "        tokenized_query = self.QueryTokens()\n",
    "        # Vector representation of the query\n",
    "        info_vector_query = self.QueryVector(tokenized_query, self.info_postings)\n",
    "        topic_vector_query = self.QueryVector(tokenized_query, self.topic_postings)\n",
    "        # Calculating Similarity Values \n",
    "        info_sim_list = self.CosineSimilarity(normalized_info_matrix, info_vector_query)\n",
    "        topic_sim_list = self.CosineSimilarity(normalized_topic_matrix, topic_vector_query)\n",
    "        # Computing Weighted Similarity Value \n",
    "        query_docs = self.DocRetrieval(info_sim_list, topic_sim_list)\n",
    "        # Returns tuple with docID, similarity value to the user sorted by similarity value for top d documents\n",
    "        return query_docs \n",
    "\n",
    "    \n",
    "    def QueryTokens(self):\n",
    "        \"\"\"Tokenizes the Query Based on the Rules for the Document Index and Posting Tokenization\"\"\"\n",
    "        modified_query = []\n",
    "        tokens_query = word_tokenize(self.query)  # Tokens of the query\n",
    "        modified_query = []\n",
    "        port_stemmer = PorterStemmer()\n",
    "        for token in tokens_query:\n",
    "            if token.isalpha() and token not in stopwords.words('english'):  # Removing stop words and punctuation \n",
    "                modified_query.append(port_stemmer.stem(token))  # Stemming the tokens\n",
    "        \n",
    "        return modified_query\n",
    "    \n",
    "    def QueryVector(self, token_query, postings):\n",
    "        \"\"\"Returns a Vector Representation of the Query Using the Postings List\"\"\"\n",
    "        vector = np.zeros(len(postings))\n",
    "        for term in token_query:  # Iterating through each query term\n",
    "            for index, key in enumerate(postings.keys()):  # Iterating through the term dictionary, aka the keys\n",
    "                if term == key:  # Finding when the query term matches the dictionary\n",
    "                    vector[index] += 1  # Adding a one to the vector location\n",
    "        return vector\n",
    "        \n",
    "    def IDF(self, index):\n",
    "        \"\"\"Returns IDF Weights for the Index\"\"\"\n",
    "        idf_lst = []\n",
    "        for term in index.keys():\n",
    "            idf_weight = np.log2(self.N/index[term]['Doc Freq']) # using log2\n",
    "            idf_lst.append(idf_weight)\n",
    "        return idf_lst            \n",
    "\n",
    "    def TFIDF(self, idf_vals, non_normalized_matrix):\n",
    "        \"\"\"Calculates the normalized postings list using TF*IDF Weights\"\"\"\n",
    "        non_normalized_matrix = np.transpose(non_normalized_matrix)\n",
    "        normalized_matrix = []\n",
    "        for row in range(len(idf_vals)):\n",
    "            new_val = np.round(idf_vals[row] * non_normalized_matrix[row], 3) # IDF * TF for each term as an array\n",
    "            normalized_val = np.round(np.sqrt(sum(idf_vals[row]**2 * non_normalized_matrix[row]**2)),3)  # Normalizing the value \n",
    "            normalized_matrix.append(new_val/normalized_val)\n",
    "        normalized_matrix = np.transpose(normalized_matrix)\n",
    "    \n",
    "        return normalized_matrix\n",
    "     \n",
    "    def VectorSpace(self, postings):\n",
    "        \"\"\"Vector Space Representation of the Postings List\"\"\"\n",
    "        # Sparse Empty Matrix\n",
    "        df = pd.DataFrame(columns = postings.keys(), index=list(range(self.N)))\n",
    "        df.fillna(value=0, inplace=True)  # Creating sparse matrix of zeros\n",
    "        \n",
    "        for term in postings.keys():  # Iterating through each term in postings list (key)\n",
    "            for occurence in postings[term]:  # Obtaining docID and freq for each term\n",
    "                doc_num = occurence[0]\n",
    "                term_freq = occurence[1]\n",
    "                df.at[doc_num, term] += term_freq  # Updating term frequency \n",
    "\n",
    "        return np.array(df)  # Returning numpy array of df\n",
    "\n",
    "    def CosineSimilarity(self, posting_matrix, vector_query):\n",
    "        \"\"\"Calculates the Cosine Similarity between each Document Vector and the Query Vector. Returns Top K Similar Documents\"\"\"\n",
    "        cosine_sim_list = []\n",
    "        for docID, doc in enumerate(posting_matrix):\n",
    "            dot_prod = np.dot(doc, vector_query)\n",
    "            x_norm = 0\n",
    "            y_norm = 0\n",
    "            \n",
    "            for val in doc:\n",
    "                x_norm += val**2\n",
    "            \n",
    "            for val in vector_query:\n",
    "                y_norm += val**2\n",
    "            \n",
    "            if x_norm * y_norm != 0:  # Catch dot prod zero errors where no terms appear\n",
    "                cosine_similarity_val = round(dot_prod/(np.sqrt(x_norm * y_norm)), 3)        \n",
    "                cosine_sim_list.append([cosine_similarity_val, docID])\n",
    "    \n",
    "        sorted_list = sorted(cosine_sim_list, reverse=True)\n",
    "        return sorted_list[0:self.k]  \n",
    "\n",
    "    def DocRetrieval(self, info_similarity, topic_similarity):\n",
    "        \"\"\"Returns the Top D Courses to the Items Based on the Weighted Similarity Measure between Topic and Info of the course\"\"\"\n",
    "        info_weight = 1 - self.topic_weight\n",
    "        topic_weight = self.topic_weight\n",
    "        \n",
    "        combined_sim_dic = {}\n",
    "        \n",
    "        for similarity in info_similarity: \n",
    "            doc_id = similarity[1]\n",
    "            sim_val = similarity[0] * info_weight\n",
    "            combined_sim_dic[doc_id] = sim_val\n",
    "            \n",
    "        for similarity in topic_similarity:  \n",
    "            doc_id = similarity[1]\n",
    "            sim_val = similarity[0] * topic_weight\n",
    "            if doc_id not in combined_sim_dic.keys():\n",
    "                combined_sim_dic[doc_id] = sim_val\n",
    "            else:\n",
    "                combined_sim_dic[doc_id] += sim_val\n",
    "        \n",
    "        return list(sorted(combined_sim_dic.items(), key=lambda item: item[1], reverse=True))[0:self.d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "922c2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 10  # Number of Similar Documents Returned to Weighting System\n",
    "#d = 8  # Number of Documents Displayed to User (Query #12 only has 9 relevant docs, so only retrieving 8 for each one.)\n",
    "# Different Topic Weights to Test\n",
    "#topic_weight_lst = np.arange(.2, .8, .1)  # Weight for the course name (Course description weight is 1-topic_weight)\n",
    "\n",
    "# Testing the System on the Queries with Different Weights \n",
    "#results = pd.DataFrame(columns = ['Topic Weight', 'Query #', 'Doc ID'])\n",
    "#for topic_weight in topic_weight_lst:\n",
    "#    x = TestQueryRetrieval(info_index, info_postings, topic_index, topic_postings, k, d, topic_weight)    \n",
    "#    for key in raw_queries.keys():\n",
    "#        query = raw_queries[key]\n",
    "#        retrieved_docs = x.RetrieveDocs(query)\n",
    "#        for docID, sim_val in retrieved_docs:\n",
    "#            results = results.append({'Topic Weight': round(topic_weight, 1), 'Query #': key, 'Doc ID': docID}, ignore_index = True)\n",
    "\n",
    "# Saving results to csv file\n",
    "#results.to_csv('test_results.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba8093",
   "metadata": {},
   "source": [
    "Above is the code used to test different article topic weights for each 30 queries. Topic weights from .2 to .8 were tested while retrieving the top 8 documents from the system. The results were saved in a pandas dataframe and later saved to the test_results.csv. Since the testing of the system took a significant amount of time, it might be best to comment out the above code chunk is the test_result.csv file is saved in the same directory as this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3486aab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relevance from http://ir.dcs.gla.ac.uk/resources/test_collections/medl/\n",
    "columns=['Query', 'Doc ID'] \n",
    "df = pd.read_table('./ML/MED.REL', sep=' ', header=None, usecols=[0,2])  # Holds the query ID and it's relevant doc IDs\n",
    "df.columns = columns\n",
    "\n",
    "# Results from system_test_query.py\n",
    "results = pd.read_csv('test_results.csv')\n",
    "topic_weights_lst = results['Topic Weight'].unique()\n",
    "\n",
    "# Going through the results and counting how many times the returned document was relevant according the MedLine test collection \n",
    "n_queries = 30\n",
    "n_retrieved = 8\n",
    "final_results = {}\n",
    "for query_num in range(1, n_queries+1):\n",
    "    ## Looking in dataframe for the specific query\n",
    "    query_df = results[results['Query #'] == query_num]\n",
    "    for weight in topic_weights_lst:  # Going through each weighting scheme\n",
    "        relevant_count = 0\n",
    "        weight = round(weight,1)\n",
    "        weight_df = query_df[query_df['Topic Weight'] == weight]\n",
    "        returned_documents = weight_df['Doc ID'].values  # Getting the documents the system returned \n",
    "        # Check if the document is relevant \n",
    "        relevant_query = df[df['Query'] == query_num]\n",
    "        relevant_docs = relevant_query['Doc ID'].values\n",
    "        \n",
    "        for doc in returned_documents:  # Looping through returned documents by system\n",
    "            if doc in relevant_docs:  # Checking if document was part of the relevant collection \n",
    "                relevant_count += 1\n",
    "        \n",
    "        # Updating results (Results are the term weight with the total number of relevant returned documents)\n",
    "        if weight not in final_results.keys():\n",
    "            final_results[weight] = relevant_count\n",
    "        else:\n",
    "            final_results[weight] += relevant_count\n",
    "\n",
    "        \n",
    "x = []  # Term Weights\n",
    "y = []  # Precision\n",
    "for key in final_results.keys():\n",
    "    x.append(key)\n",
    "    y.append(final_results[key]/(n_queries*n_retrieved))  # Dividing by number of queries and number of documents returned to get precision\n",
    "\n",
    "\n",
    "# Plot of the results \n",
    "sns.barplot(x= x, y=y)\n",
    "plt.xlabel('Topic Weight')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Query Retrieval Test Accuracy for Multiple Topic Weights')\n",
    "plt.savefig('weights_image.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29187a",
   "metadata": {},
   "source": [
    "Above is a display of the results obtained from the MedLine testing data. As seen above a topic weight of .2 resulted in the highest precision value of 41%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2766426",
   "metadata": {},
   "source": [
    "## Results <a id=Results>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6904f39",
   "metadata": {},
   "source": [
    "The results and implementation of this information retrieval system resulted in a course retrieval system without any of the three limitations mentioned above. The system was able to provide courses relevant to the user using a weighted similarity measurement based on the course’s name and description. Additionally it offered the ability to search multiple topics and provide relevant information about them. That being said there still are a few limitations for further development. One big limitation is the sample size of the documents. For now the course’s indexed are ones only part of that are in the Data Science Computational Methods course catalog. Also, there is no way to filter the courses based on the time of the year the class is offered or for keyword analysis on different synonyms or similar phrases. Further research needs to be done to validate the appropriate value for the course topic weight parameter. Relevance feedback could be used in the future to help approximate this value. Future steps include creating a server side for the index/dictionary and postings list based on their relational database structure. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
